{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44eed049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 124, 124, 6)       156       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 62, 62, 6)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 58, 58, 16)        2416      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 29, 29, 16)        0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 13456)             0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 120)               1614840   \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 84)                10164     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 53)                4505      \n",
      "=================================================================\n",
      "Total params: 1,632,081\n",
      "Trainable params: 1,632,081\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Found 74200 images belonging to 53 classes.\n",
      "Found 24592 images belonging to 53 classes.\n",
      "Epoch 1/20\n",
      "7420/7420 [==============================] - 363s 49ms/step - loss: 1.3810 - accuracy: 0.6205 - val_loss: 0.0991 - val_accuracy: 0.9654\n",
      "Epoch 2/20\n",
      "7420/7420 [==============================] - 357s 48ms/step - loss: 0.1921 - accuracy: 0.9358 - val_loss: 0.0632 - val_accuracy: 0.9782\n",
      "Epoch 3/20\n",
      "7420/7420 [==============================] - 358s 48ms/step - loss: 0.1191 - accuracy: 0.9607 - val_loss: 0.0361 - val_accuracy: 0.9884\n",
      "Epoch 4/20\n",
      "7420/7420 [==============================] - 354s 48ms/step - loss: 0.0904 - accuracy: 0.9703 - val_loss: 0.0226 - val_accuracy: 0.9933\n",
      "Epoch 5/20\n",
      "7420/7420 [==============================] - 350s 47ms/step - loss: 0.0748 - accuracy: 0.9768 - val_loss: 0.0207 - val_accuracy: 0.9935\n",
      "Epoch 6/20\n",
      "7420/7420 [==============================] - 358s 48ms/step - loss: 0.0659 - accuracy: 0.9801 - val_loss: 0.0453 - val_accuracy: 0.9870\n",
      "Epoch 7/20\n",
      "7420/7420 [==============================] - 367s 49ms/step - loss: 0.0626 - accuracy: 0.9812 - val_loss: 0.0182 - val_accuracy: 0.9946\n",
      "Epoch 8/20\n",
      "7420/7420 [==============================] - 377s 51ms/step - loss: 0.0538 - accuracy: 0.9839 - val_loss: 0.0126 - val_accuracy: 0.9961\n",
      "Epoch 9/20\n",
      "7420/7420 [==============================] - 353s 48ms/step - loss: 0.0501 - accuracy: 0.9849 - val_loss: 0.0192 - val_accuracy: 0.9948\n",
      "Epoch 10/20\n",
      "7420/7420 [==============================] - 351s 47ms/step - loss: 0.0536 - accuracy: 0.9843 - val_loss: 0.0174 - val_accuracy: 0.9953\n",
      "Epoch 11/20\n",
      "7420/7420 [==============================] - 353s 48ms/step - loss: 0.0464 - accuracy: 0.9862 - val_loss: 0.0107 - val_accuracy: 0.9968\n",
      "Epoch 12/20\n",
      "7420/7420 [==============================] - 362s 49ms/step - loss: 0.0466 - accuracy: 0.9865 - val_loss: 0.0182 - val_accuracy: 0.9952\n",
      "Epoch 13/20\n",
      "7420/7420 [==============================] - 359s 48ms/step - loss: 0.0442 - accuracy: 0.9876 - val_loss: 0.0244 - val_accuracy: 0.9940\n",
      "Epoch 14/20\n",
      "7420/7420 [==============================] - 759s 102ms/step - loss: 0.0520 - accuracy: 0.9861 - val_loss: 0.0083 - val_accuracy: 0.9979\n",
      "Epoch 15/20\n",
      "7420/7420 [==============================] - 518s 70ms/step - loss: 0.0480 - accuracy: 0.9874 - val_loss: 0.0135 - val_accuracy: 0.9964\n",
      "Epoch 16/20\n",
      "7420/7420 [==============================] - 354s 48ms/step - loss: 0.0454 - accuracy: 0.9883 - val_loss: 0.0115 - val_accuracy: 0.9974\n",
      "Epoch 17/20\n",
      "7420/7420 [==============================] - 353s 48ms/step - loss: 0.0430 - accuracy: 0.9886 - val_loss: 0.0161 - val_accuracy: 0.9950\n",
      "Epoch 18/20\n",
      "7420/7420 [==============================] - 355s 48ms/step - loss: 0.0451 - accuracy: 0.9888 - val_loss: 0.0213 - val_accuracy: 0.9946\n",
      "Epoch 19/20\n",
      "7420/7420 [==============================] - 354s 48ms/step - loss: 0.0398 - accuracy: 0.9899 - val_loss: 0.0187 - val_accuracy: 0.9951\n",
      "Epoch 20/20\n",
      "7420/7420 [==============================] - 355s 48ms/step - loss: 0.0439 - accuracy: 0.9900 - val_loss: 0.0351 - val_accuracy: 0.9928\n",
      "Model Saved\n",
      "Weights saved\n"
     ]
    }
   ],
   "source": [
    "# Importing the Keras libraries and packages\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Convolution2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense , Dropout\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "sz = 128\n",
    "# Step 1 - Building the CNN\n",
    "\n",
    "# Initializing the CNN\n",
    "classifier = Sequential()\n",
    "\n",
    "# First convolutional layer and pooling\n",
    "classifier.add(Convolution2D(6, (5, 5), activation='relu', input_shape=(sz, sz, 1)))\n",
    "classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Second convolutional layer and pooling\n",
    "classifier.add(Convolution2D(16, (5, 5), activation='relu'))\n",
    "classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Flatten the layers\n",
    "classifier.add(Flatten())\n",
    "\n",
    "# Fully connected layers\n",
    "classifier.add(Dense(units=120, activation='relu'))\n",
    "classifier.add(Dense(units=84, activation='relu'))\n",
    "classifier.add(Dense(units=53, activation='softmax'))\n",
    "\n",
    "# # First convolution layer and pooling\n",
    "# classifier.add(Convolution2D(32, (3, 3), input_shape=(sz, sz, 1), activation='relu'))\n",
    "# classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# # Second convolution layer and pooling\n",
    "# classifier.add(Convolution2D(32, (3, 3), activation='relu'))\n",
    "# # input_shape is going to be the pooled feature maps from the previous convolution layer\n",
    "# classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# #classifier.add(Convolution2D(32, (3, 3), activation='relu'))\n",
    "# # input_shape is going to be the pooled feature maps from the previous convolution layer\n",
    "# #classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# # Flattening the layers\n",
    "# classifier.add(Flatten())\n",
    "\n",
    "# # Adding a fully connected layer\n",
    "# classifier.add(Dense(units=128, activation='relu'))\n",
    "# classifier.add(Dropout(0.40))\n",
    "# classifier.add(Dense(units=96, activation='relu'))\n",
    "# classifier.add(Dropout(0.40))\n",
    "# classifier.add(Dense(units=64, activation='relu'))\n",
    "# classifier.add(Dense(units=27, activation='softmax')) # softmax for more than 2\n",
    "\n",
    "# Compiling the CNN\n",
    "classifier.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy']) # categorical_crossentropy for more than 2\n",
    "\n",
    "\n",
    "# Step 2 - Preparing the train/test data and training the model\n",
    "classifier.summary()\n",
    "# Code copied from - https://keras.io/preprocessing/image/\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "training_set = train_datagen.flow_from_directory(\"E:/VSCR_ASL/WITHOUT_FLIP/datamixed/train\",\n",
    "                                                 target_size=(sz, sz),\n",
    "                                                 batch_size=10,\n",
    "                                                 color_mode='grayscale',\n",
    "                                                 class_mode='categorical')\n",
    "# class_indices = training_set.class_indices\n",
    "# labels = list(class_indices.keys())\n",
    "# print(labels)\n",
    "\n",
    "test_set = test_datagen.flow_from_directory(\"E:/VSCR_ASL/WITHOUT_FLIP/datamixed/test\",\n",
    "                                            target_size=(sz , sz),\n",
    "                                            batch_size=10,\n",
    "                                            color_mode='grayscale',\n",
    "                                            class_mode='categorical')\n",
    "# class_indices = test_set.class_indices\n",
    "# labels = list(class_indices.keys())\n",
    "# print(labels)\n",
    "\n",
    "history= classifier.fit(\n",
    "        training_set,\n",
    "        steps_per_epoch=74200/10, # No of images in training set\n",
    "        epochs=20,\n",
    "        validation_data=test_set,\n",
    "        validation_steps=24592/10) # No of images in test set\n",
    "\n",
    "# #confusion matrix\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# # make predictions on test data\n",
    "# y_pred = model_json.predict(X_test)\n",
    "\n",
    "# # compute confusion matrix\n",
    "# cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# # plot confusion matrix\n",
    "# sns.heatmap(cm, annot=True, cmap='Blues')\n",
    "# plt.xlabel('Predicted labels')\n",
    "# plt.ylabel('True labels')\n",
    "# plt.show()\n",
    "\n",
    "# Saving the model\n",
    "model_json = classifier.to_json()\n",
    "with open(\"MODEL_WF-mixed_V4.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "print('Model Saved')\n",
    "classifier.save_weights('MODEL_WF-mixed_V4.h5')\n",
    "print('Weights saved')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8ec69f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
